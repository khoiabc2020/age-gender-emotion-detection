{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Multi-Dataset Training for Kaggle - Target 80-85%\n",
        "\n",
        "## ðŸ“‹ Configuration\n",
        "- **Datasets:** FER2013 + UTKFace + RAF-DB + (Optional 4th)\n",
        "- **Model:** EfficientNet-B0\n",
        "- **Target:** 80-85% accuracy (3 datasets) or 81-86% (4 datasets)\n",
        "- **Time:** 10-13 hours (GPU P100)\n",
        "\n",
        "## âš™ï¸ Pre-requisites\n",
        "1. **Enable GPU:** Runtime > Change runtime type > GPU (P100)\n",
        "2. **Add 3-4 datasets via \"+ Add Input\":**\n",
        "   - `msambare/fer2013` **(Required)**\n",
        "   - `jangedoo/utkface-new` **(Required)**\n",
        "   - `shuvoalok/raf-db-dataset` **(Required)**\n",
        "   - `davilsena/ckextended` **(Optional - Recommended)**\n",
        "\n",
        "## â–¶ï¸ Instructions\n",
        "**Run cells in order: 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ 6 â†’ 7 â†’ 8 â†’ 9**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Š **Expected Results:**\n",
        "- **3 datasets (FER2013 + UTKFace + RAF-DB):** 80-85% âœ“ **RECOMMENDED**\n",
        "- **4 datasets (+ CK+):** 81-86% âœ“ **BETTER**\n",
        "\n",
        "## âš ï¸ **Important Notes:**\n",
        "- **AffectNet** and **FER2013+** are no longer available on Kaggle (404)\n",
        "- **3 datasets is enough** to reach production-ready accuracy (80-85%)\n",
        "- **CK+ Extended** is the best available 4th dataset option\n",
        "- Training works perfectly with 3 datasets - no need to wait for 4th!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 1: CHECK GPU\n",
        "\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECKING GPU\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {cuda.is_available()}\")\n",
        "\n",
        "if cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    print(\"\\n[OK] GPU is ready!\")\n",
        "else:\n",
        "    print(\"\\n[WARNING] No GPU available! Training will be very slow.\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 2: CLONE REPOSITORY\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CLONING REPOSITORY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "repo_url = \"https://github.com/khoiabc2020/age-gender-emotion-detection.git\"\n",
        "repo_dir = Path(\"/kaggle/working/repo\")\n",
        "\n",
        "if repo_dir.exists():\n",
        "    print(\"\\n[INFO] Repository exists, pulling latest changes...\")\n",
        "    !cd /kaggle/working/repo && git pull\n",
        "    print(\"[OK] Updated to latest version\")\n",
        "else:\n",
        "    print(\"\\n[INFO] Cloning repository...\")\n",
        "    !git clone {repo_url} /kaggle/working/repo\n",
        "    print(\"[OK] Repository cloned\")\n",
        "\n",
        "%cd /kaggle/working/repo/training_experiments\n",
        "\n",
        "print(f\"\\n[OK] Working directory: {os.getcwd()}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 3: CHECK 4 DATASETS\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CHECKING 4 DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "dataset_paths = {}\n",
        "\n",
        "# 1. FER2013 - Main emotion dataset (28K images)\n",
        "print(\"\\n[1/4] Checking FER2013...\")\n",
        "fer_paths = [\n",
        "    '/kaggle/input/fer2013',\n",
        "    '/kaggle/input/msambare-fer2013'\n",
        "]\n",
        "for path in fer_paths:\n",
        "    if Path(path).exists():\n",
        "        dataset_paths['fer2013'] = path\n",
        "        print(f\"  [OK] FER2013: {path}\")\n",
        "        break\n",
        "if 'fer2013' not in dataset_paths:\n",
        "    print(\"  [ERROR] FER2013 not found! This is required.\")\n",
        "\n",
        "# 2. UTKFace - Age/Gender dataset (23K images)\n",
        "print(\"\\n[2/4] Checking UTKFace...\")\n",
        "utk_paths = [\n",
        "    '/kaggle/input/utkface-new',\n",
        "    '/kaggle/input/jangedoo-utkface-new'\n",
        "]\n",
        "for path in utk_paths:\n",
        "    if Path(path).exists():\n",
        "        dataset_paths['utkface'] = path\n",
        "        print(f\"  [OK] UTKFace: {path}\")\n",
        "        break\n",
        "if 'utkface' not in dataset_paths:\n",
        "    print(\"  [WARN] UTKFace not found\")\n",
        "    print(\"  [INFO] Add via: + Add Input -> Search 'jangedoo/utkface-new'\")\n",
        "\n",
        "# 3. RAF-DB - High-quality emotion dataset (12K images)\n",
        "print(\"\\n[3/4] Checking RAF-DB...\")\n",
        "rafdb_paths = [\n",
        "    '/kaggle/input/raf-db-dataset',\n",
        "    '/kaggle/input/shuvoalok-raf-db-dataset',\n",
        "    '/kaggle/input/raf-db',\n",
        "    '/kaggle/input/alex1233213-raf-db'\n",
        "]\n",
        "for path in rafdb_paths:\n",
        "    if Path(path).exists():\n",
        "        dataset_paths['rafdb'] = path\n",
        "        print(f\"  [OK] RAF-DB: {path}\")\n",
        "        break\n",
        "if 'rafdb' not in dataset_paths:\n",
        "    print(\"  [WARN] RAF-DB not found\")\n",
        "    print(\"  [INFO] Add via: + Add Input -> Search 'shuvoalok/raf-db-dataset'\")\n",
        "\n",
        "# 4. Additional Dataset - Try multiple alternatives\n",
        "print(\"\\n[4/4] Checking Additional Datasets...\")\n",
        "\n",
        "found_4th = False\n",
        "\n",
        "# Try CK+ Extended (Most reliable alternative)\n",
        "if not found_4th:\n",
        "    ckplus_paths = [\n",
        "        '/kaggle/input/ckextended',\n",
        "        '/kaggle/input/davilsena-ckextended',\n",
        "        '/kaggle/input/ck-extended',\n",
        "        '/kaggle/input/ckplus',\n",
        "        '/kaggle/input/ck-plus'\n",
        "    ]\n",
        "    for path in ckplus_paths:\n",
        "        if Path(path).exists():\n",
        "            dataset_paths['ckplus'] = path\n",
        "            print(f\"  [OK] CK+ Extended: {path}\")\n",
        "            found_4th = True\n",
        "            break\n",
        "\n",
        "# Try FER2013+ (May be unavailable)\n",
        "if not found_4th:\n",
        "    ferplus_paths = [\n",
        "        '/kaggle/input/ferplus',\n",
        "        '/kaggle/input/shreyanshverma27-ferplus',\n",
        "        '/kaggle/input/fer-plus',\n",
        "        '/kaggle/input/fer2013plus',\n",
        "        '/kaggle/input/fer2013-plus'\n",
        "    ]\n",
        "    for path in ferplus_paths:\n",
        "        if Path(path).exists():\n",
        "            dataset_paths['ferplus'] = path\n",
        "            print(f\"  [OK] FER2013+ (FERPlus): {path}\")\n",
        "            found_4th = True\n",
        "            break\n",
        "\n",
        "# Try JAFFE\n",
        "if not found_4th:\n",
        "    jaffe_paths = [\n",
        "        '/kaggle/input/jaffe',\n",
        "        '/kaggle/input/jaffe-dataset',\n",
        "        '/kaggle/input/japanese-female-facial-expression'\n",
        "    ]\n",
        "    for path in jaffe_paths:\n",
        "        if Path(path).exists():\n",
        "            dataset_paths['jaffe'] = path\n",
        "            print(f\"  [OK] JAFFE: {path}\")\n",
        "            found_4th = True\n",
        "            break\n",
        "\n",
        "# Try EmotioNet or other emotion datasets\n",
        "if not found_4th:\n",
        "    other_paths = [\n",
        "        '/kaggle/input/emotionet',\n",
        "        '/kaggle/input/emotion-net',\n",
        "        '/kaggle/input/facial-expressions',\n",
        "        '/kaggle/input/expw'\n",
        "    ]\n",
        "    for path in other_paths:\n",
        "        if Path(path).exists():\n",
        "            dataset_paths['other'] = path\n",
        "            print(f\"  [OK] Additional dataset: {path}\")\n",
        "            found_4th = True\n",
        "            break\n",
        "\n",
        "# If none found - that's OK!\n",
        "if not found_4th:\n",
        "    print(\"  [INFO] No 4th dataset found - will train with 3 datasets\")\n",
        "    print(\"  [INFO] Recommended: Add CK+ Extended for better results\")\n",
        "    print(\"    â†’ Search: 'davilsena/ckextended' in Kaggle\")\n",
        "    print(\"  [INFO] 3 datasets is enough for 80-85% accuracy!\")\n",
        "\n",
        "# Save paths\n",
        "paths_file = '/kaggle/working/dataset_paths.json'\n",
        "with open(paths_file, 'w') as f:\n",
        "    json.dump(dataset_paths, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"DATASETS READY: {len(dataset_paths)}/4\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, path in dataset_paths.items():\n",
        "    print(f\"  {name.upper()}: {path}\")\n",
        "\n",
        "print(f\"\\n[INFO] Paths saved to: {paths_file}\")\n",
        "\n",
        "# Estimate total images\n",
        "estimates = {\n",
        "    'fer2013': 28709,\n",
        "    'utkface': 23708,\n",
        "    'rafdb': 12271,\n",
        "    'ckplus': 10000,\n",
        "    'ferplus': 35887,\n",
        "    'jaffe': 5000,\n",
        "    'expw': 15000,\n",
        "    'other': 10000\n",
        "}\n",
        "\n",
        "total_estimate = sum(estimates[name] for name in dataset_paths.keys() if name in estimates)\n",
        "print(f\"\\n[ESTIMATE] Total images: ~{total_estimate:,}\")\n",
        "\n",
        "if len(dataset_paths) >= 3:\n",
        "    print(\"\\n[SUCCESS] Ready for high-accuracy training!\")\n",
        "    print(f\"Expected accuracy: 80-85%\")\n",
        "elif len(dataset_paths) >= 2:\n",
        "    print(\"\\n[OK] Ready for training with 2 datasets\")\n",
        "    print(f\"Expected accuracy: 77-82%\")\n",
        "else:\n",
        "    print(\"\\n[WARN] Only 1 dataset found\")\n",
        "    print(f\"Expected accuracy: 75-80%\")\n",
        "    print(\"\\nTo reach 80-85%, please add more datasets via '+ Add Input'\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 4: INSTALL DEPENDENCIES\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INSTALLING DEPENDENCIES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n[INFO] Installing packages...\")\n",
        "print(\"[INFO] Time: ~2-3 minutes\\n\")\n",
        "\n",
        "%pip install -q timm albumentations tensorboard onnx onnxscript onnxruntime torchmetrics opencv-python\n",
        "\n",
        "print(\"\\n[OK] All dependencies installed!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 5: TRAIN WITH 4 DATASETS (Main Training - 10-12 hours)\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic')\n",
        "warnings.filterwarnings('ignore', message='.*UnsupportedFieldAttributeWarning.*')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"4-DATASET TRAINING - TARGET 80-85%\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 120\n",
        "BATCH_SIZE = 48\n",
        "LEARNING_RATE = 0.00015\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EARLY_STOPPING_PATIENCE = 35\n",
        "WARMUP_EPOCHS = 5\n",
        "\n",
        "print(f\"\\n[CONFIG] Training Configuration:\")\n",
        "print(f\"  Model: EfficientNet-B0\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Max Epochs: {EPOCHS}\")\n",
        "print(f\"  Early Stop Patience: {EARLY_STOPPING_PATIENCE}\")\n",
        "print(f\"  Device: {DEVICE} ({torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'})\")\n",
        "\n",
        "# Load dataset paths\n",
        "with open('/kaggle/working/dataset_paths.json') as f:\n",
        "    dataset_paths = json.load(f)\n",
        "\n",
        "print(f\"\\n[INFO] Found {len(dataset_paths)} datasets:\")\n",
        "for name in dataset_paths.keys():\n",
        "    print(f\"  - {name.upper()}\")\n",
        "\n",
        "# Data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.25, contrast=0.25),\n",
        "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.25)),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load all datasets\n",
        "print(\"\\n[INFO] Loading datasets...\")\n",
        "\n",
        "all_train_datasets = []\n",
        "all_test_datasets = []\n",
        "total_train = 0\n",
        "total_test = 0\n",
        "\n",
        "for name, path in dataset_paths.items():\n",
        "    dataset_path = Path(path)\n",
        "    train_dir = None\n",
        "    test_dir = None\n",
        "    \n",
        "    if (dataset_path / 'train').exists():\n",
        "        train_dir = dataset_path / 'train'\n",
        "        test_dir = dataset_path / 'test' if (dataset_path / 'test').exists() else train_dir\n",
        "    else:\n",
        "        train_dirs = list(dataset_path.glob('**/train'))\n",
        "        if train_dirs:\n",
        "            train_dir = train_dirs[0]\n",
        "            test_dir = train_dir.parent / 'test'\n",
        "            if not test_dir.exists():\n",
        "                test_dir = train_dir.parent / 'validation' if (train_dir.parent / 'validation').exists() else train_dir\n",
        "    \n",
        "    if train_dir and train_dir.exists():\n",
        "        try:\n",
        "            train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "            test_ds = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "            \n",
        "            if len(train_ds.classes) > 0 and len(train_ds) > 0:\n",
        "                all_train_datasets.append(train_ds)\n",
        "                all_test_datasets.append(test_ds)\n",
        "                total_train += len(train_ds)\n",
        "                total_test += len(test_ds)\n",
        "                print(f\"  [OK] {name.upper()}: {len(train_ds):,} train, {len(test_ds):,} test, {len(train_ds.classes)} classes\")\n",
        "        except Exception as e:\n",
        "            print(f\"  [WARN] {name.upper()}: Failed - {str(e)[:50]}\")\n",
        "    else:\n",
        "        print(f\"  [WARN] {name.upper()}: train directory not found\")\n",
        "\n",
        "if len(all_train_datasets) == 0:\n",
        "    print(\"\\n[ERROR] No datasets loaded!\")\n",
        "    raise RuntimeError(\"No valid datasets found. Check '+ Add Input'\")\n",
        "\n",
        "train_dataset = ConcatDataset(all_train_datasets) if len(all_train_datasets) > 1 else all_train_datasets[0]\n",
        "test_dataset = ConcatDataset(all_test_datasets) if len(all_test_datasets) > 1 else all_test_datasets[0]\n",
        "\n",
        "print(f\"\\n[SUCCESS] Combined datasets:\")\n",
        "print(f\"  Total train: {total_train:,} images\")\n",
        "print(f\"  Total test: {total_test:,} images\")\n",
        "print(f\"  Datasets used: {len(all_train_datasets)}\")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True,\n",
        "    drop_last=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "first_dataset = all_train_datasets[0]\n",
        "num_classes = len(first_dataset.classes) if hasattr(first_dataset, 'classes') else 7\n",
        "class_names = first_dataset.classes if hasattr(first_dataset, 'classes') else [f'class_{i}' for i in range(num_classes)]\n",
        "\n",
        "print(f\"[OK] Classes: {num_classes}\")\n",
        "print(f\"[OK] Batches: {len(train_loader)} train, {len(test_loader)} test\")\n",
        "\n",
        "# Model\n",
        "print(\"\\n[INFO] Creating EfficientNet-B0 with dropout=0.6...\")\n",
        "model = timm.create_model('efficientnet_b0', pretrained=True, num_classes=num_classes, drop_rate=0.6)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"[OK] Model ready ({total_params:,} parameters)\")\n",
        "\n",
        "# Loss function\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2.5, label_smoothing=0.2):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.label_smoothing = label_smoothing\n",
        "        \n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)(inputs, targets)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss\n",
        "\n",
        "criterion = FocalLoss(alpha=1, gamma=2.5, label_smoothing=0.2)\n",
        "print(\"[OK] Focal Loss (gamma=2.5, smoothing=0.2)\")\n",
        "\n",
        "# Optimizer & scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.03, betas=(0.9, 0.999))\n",
        "\n",
        "def get_lr_scheduler(optimizer, warmup_epochs, total_epochs):\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < warmup_epochs:\n",
        "            return (epoch + 1) / warmup_epochs\n",
        "        else:\n",
        "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
        "            return 0.5 * (1 + np.cos(np.pi * progress))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_lr_scheduler(optimizer, WARMUP_EPOCHS, EPOCHS)\n",
        "print(\"[OK] AdamW + Cosine Annealing with Warmup\")\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.3):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "print(\"[OK] Mixup (alpha=0.3)\")\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(f\"Expected: 80-85% with {len(all_train_datasets)} datasets\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "best_accuracy = 0\n",
        "best_epoch = 0\n",
        "patience_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "history = {'train_loss': [], 'train_acc': [], 'test_acc': [], 'lr': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # TRAIN\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    \n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
        "    for batch_idx, (images, labels) in enumerate(pbar):\n",
        "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "        \n",
        "        if np.random.rand() > 0.4:\n",
        "            images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=0.3)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)\n",
        "        else:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        train_total += labels.size(0)\n",
        "        train_correct += predicted.eq(labels).sum().item()\n",
        "        \n",
        "        pbar.set_postfix({'loss': f'{train_loss/(batch_idx+1):.4f}', 'acc': f'{100.*train_correct/train_total:.2f}%'})\n",
        "    \n",
        "    train_acc = 100. * train_correct / train_total\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    \n",
        "    # VALIDATION\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    test_acc = 100. * test_correct / test_total\n",
        "    \n",
        "    scheduler.step()\n",
        "    \n",
        "    history['train_loss'].append(float(avg_train_loss))\n",
        "    history['train_acc'].append(float(train_acc))\n",
        "    history['test_acc'].append(float(test_acc))\n",
        "    history['lr'].append(float(current_lr))\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\nEpoch {epoch+1}: Loss={avg_train_loss:.4f}, Train={train_acc:.2f}%, Val={test_acc:.2f}%, LR={current_lr:.7f}, Time={elapsed/60:.1f}m\")\n",
        "    \n",
        "    # Save best model\n",
        "    if test_acc > best_accuracy:\n",
        "        best_accuracy = test_acc\n",
        "        best_epoch = epoch + 1\n",
        "        patience_counter = 0\n",
        "        \n",
        "        save_dir = Path('/kaggle/working/checkpoints_production')\n",
        "        save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_accuracy': best_accuracy,\n",
        "            'class_names': class_names,\n",
        "            'num_classes': num_classes,\n",
        "            'datasets_used': list(dataset_paths.keys()),\n",
        "            'config': {\n",
        "                'batch_size': BATCH_SIZE,\n",
        "                'learning_rate': LEARNING_RATE,\n",
        "                'model': 'efficientnet_b0',\n",
        "                'dropout': 0.6,\n",
        "                'num_datasets': len(dataset_paths)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        torch.save(checkpoint, save_dir / 'best_model_4datasets.pth')\n",
        "        print(f\"[NEW BEST] {best_accuracy:.2f}% saved!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement: {patience_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
        "    \n",
        "    if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "        print(f\"\\n[STOP] Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "# Save results\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "results = {\n",
        "    'best_accuracy': float(best_accuracy),\n",
        "    'best_epoch': int(best_epoch),\n",
        "    'total_epochs': epoch + 1,\n",
        "    'training_time_hours': float(total_time/3600),\n",
        "    'num_classes': num_classes,\n",
        "    'class_names': class_names,\n",
        "    'datasets_used': list(dataset_paths.keys()),\n",
        "    'num_datasets': len(dataset_paths),\n",
        "    'total_train_images': total_train,\n",
        "    'total_test_images': total_test,\n",
        "    'history': history,\n",
        "    'config': {\n",
        "        'model': 'efficientnet_b0',\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'learning_rate': LEARNING_RATE,\n",
        "        'dropout': 0.6,\n",
        "        'label_smoothing': 0.2,\n",
        "        'focal_gamma': 2.5,\n",
        "        'num_datasets': len(dataset_paths)\n",
        "    }\n",
        "}\n",
        "\n",
        "save_dir = Path('/kaggle/working/checkpoints_production')\n",
        "save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "results_path = save_dir / 'training_results.json'\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nBest Accuracy: {best_accuracy:.2f}%\")\n",
        "print(f\"Best Epoch: {best_epoch}/{epoch+1}\")\n",
        "print(f\"Datasets Used: {len(dataset_paths)}\")\n",
        "print(f\"Total Images: {total_train:,} train + {total_test:,} test\")\n",
        "print(f\"Training Time: {total_time/3600:.2f} hours\")\n",
        "\n",
        "if best_accuracy >= 80:\n",
        "    print(\"\\n[SUCCESS] TARGET ACHIEVED! (80-85%)\")\n",
        "    print(\"Model is production-ready!\")\n",
        "elif best_accuracy >= 78:\n",
        "    print(\"\\n[EXCELLENT] Very close! (78-80%)\")\n",
        "    print(\"Model is near-production ready!\")\n",
        "elif best_accuracy >= 75:\n",
        "    print(\"\\n[GOOD] Good performance! (75-78%)\")\n",
        "else:\n",
        "    print(f\"\\n[OK] Completed with {best_accuracy:.2f}%\")\n",
        "\n",
        "print(f\"\\n[SAVED] Model: {save_dir / 'best_model_4datasets.pth'}\")\n",
        "print(f\"[SAVED] Results: {results_path}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 6: CHECK RESULTS\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results_file = Path('/kaggle/working/checkpoints_production/training_results.json')\n",
        "\n",
        "if results_file.exists():\n",
        "    with open(results_file) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(f\"\\n[SUCCESS] Training Completed!\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print('='*60)\n",
        "    print(f\"Best Accuracy: {results.get('best_accuracy', 0):.2f}%\")\n",
        "    print(f\"Best Epoch: {results.get('best_epoch', 'N/A')}\")\n",
        "    print(f\"Total Epochs: {results.get('total_epochs', 'N/A')}\")\n",
        "    \n",
        "    if 'training_time_hours' in results:\n",
        "        print(f\"Training Time: {results['training_time_hours']:.2f} hours\")\n",
        "    \n",
        "    if 'datasets_used' in results:\n",
        "        print(f\"\\nDatasets Used: {len(results['datasets_used'])}\")\n",
        "        for ds in results['datasets_used']:\n",
        "            print(f\"  - {ds.upper()}\")\n",
        "    \n",
        "    if 'total_train_images' in results:\n",
        "        print(f\"\\nTotal Images:\")\n",
        "        print(f\"  Train: {results['total_train_images']:,}\")\n",
        "        print(f\"  Test: {results['total_test_images']:,}\")\n",
        "    \n",
        "    # Evaluation\n",
        "    best_acc = results.get('best_accuracy', 0)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"EVALUATION\")\n",
        "    print('='*60)\n",
        "    \n",
        "    if best_acc >= 80:\n",
        "        print(\"[SUCCESS] TARGET ACHIEVED! (80-85%)\")\n",
        "        print(\"Model is PRODUCTION-READY!\")\n",
        "    elif best_acc >= 78:\n",
        "        print(\"[EXCELLENT] Very close! (78-80%)\")\n",
        "        print(\"Model is near-production ready!\")\n",
        "    elif best_acc >= 75:\n",
        "        print(\"[GOOD] Good performance! (75-78%)\")\n",
        "        print(\"Model can be used in production with monitoring\")\n",
        "    elif best_acc >= 70:\n",
        "        print(\"[OK] Decent performance (70-75%)\")\n",
        "        print(\"Consider adding more data or training longer\")\n",
        "    else:\n",
        "        print(f\"[INFO] Completed with {best_acc:.2f}%\")\n",
        "    \n",
        "    print(f\"\\n[INFO] Results file: {results_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n[ERROR] Results file not found!\")\n",
        "    print(\"[INFO] Training may still be in progress or failed\")\n",
        "    print(\"[INFO] Check Cell 5 output for errors\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 7: EXPORT TO ONNX\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"EXPORTING TO ONNX\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "checkpoint_path = Path('/kaggle/working/checkpoints_production/best_model_4datasets.pth')\n",
        "\n",
        "if checkpoint_path.exists():\n",
        "    print(f\"\\n[INFO] Loading checkpoint...\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    \n",
        "    import timm\n",
        "    num_classes = checkpoint['num_classes']\n",
        "    model = timm.create_model('efficientnet_b0', pretrained=False, num_classes=num_classes)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"[OK] Model loaded (acc: {checkpoint['best_accuracy']:.2f}%)\")\n",
        "    \n",
        "    # Test forward pass\n",
        "    dummy = torch.randn(1, 3, 224, 224)\n",
        "    \n",
        "    print(\"\\n[INFO] Testing forward pass...\")\n",
        "    with torch.no_grad():\n",
        "        output = model(dummy)\n",
        "        print(f\"[OK] Output shape: {output.shape}\")\n",
        "    \n",
        "    # Export to ONNX\n",
        "    onnx_path = checkpoint_path.parent / 'best_model.onnx'\n",
        "    \n",
        "    print(f\"\\n[INFO] Exporting to: {onnx_path.name}\")\n",
        "    \n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy,\n",
        "        onnx_path,\n",
        "        export_params=True,\n",
        "        opset_version=11,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={\n",
        "            'input': {0: 'batch_size'},\n",
        "            'output': {0: 'batch_size'}\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    size_mb = onnx_path.stat().st_size / (1024*1024)\n",
        "    print(f\"[OK] ONNX exported! Size: {size_mb:.1f} MB\")\n",
        "    \n",
        "    # Verify ONNX\n",
        "    print(\"\\n[INFO] Verifying ONNX model...\")\n",
        "    import onnx\n",
        "    onnx_model = onnx.load(str(onnx_path))\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "    print(\"[OK] ONNX model is valid!\")\n",
        "    \n",
        "    # Test with ONNX Runtime\n",
        "    print(\"\\n[INFO] Testing with ONNX Runtime...\")\n",
        "    import onnxruntime as ort\n",
        "    import numpy as np\n",
        "    \n",
        "    session = ort.InferenceSession(str(onnx_path))\n",
        "    test_input = np.random.randn(1, 3, 224, 224).astype(np.float32)\n",
        "    ort_outputs = session.run(None, {'input': test_input})\n",
        "    \n",
        "    print(f\"[OK] ONNX Runtime test passed!\")\n",
        "    print(f\"[INFO] Output shape: {ort_outputs[0].shape}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"[SUCCESS] MODEL READY FOR DEPLOYMENT!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nFiles:\")\n",
        "    print(f\"  PyTorch: {checkpoint_path.name} ({checkpoint_path.stat().st_size/(1024*1024):.1f} MB)\")\n",
        "    print(f\"  ONNX: {onnx_path.name} ({size_mb:.1f} MB)\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n[ERROR] Checkpoint not found!\")\n",
        "    print(\"[INFO] Please complete training (Cell 5) first\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 8: DOWNLOAD FILES\n",
        "\n",
        "from IPython.display import FileLink, display\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DOWNLOAD TRAINED MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "checkpoint_dir = Path('/kaggle/working/checkpoints_production')\n",
        "\n",
        "if not checkpoint_dir.exists():\n",
        "    print(\"[ERROR] Checkpoint directory not found!\")\n",
        "    print(\"[INFO] Please complete training (Cell 5) first\")\n",
        "else:\n",
        "    files = list(checkpoint_dir.glob('*'))\n",
        "    \n",
        "    if not files:\n",
        "        print(\"[WARN] No files found in checkpoint directory\")\n",
        "    else:\n",
        "        print(f\"\\n[INFO] Found {len(files)} files:\\n\")\n",
        "        \n",
        "        total_size = 0\n",
        "        for file_path in files:\n",
        "            if file_path.is_file():\n",
        "                size = file_path.stat().st_size / (1024*1024)\n",
        "                total_size += size\n",
        "                print(f\"{file_path.name} ({size:.1f} MB):\")\n",
        "                display(FileLink(str(file_path)))\n",
        "                print()\n",
        "        \n",
        "        print(f\"Total size: {total_size:.1f} MB\")\n",
        "        print(\"\\n[INFO] Click links above to download!\")\n",
        "        print(\"\\n[NEXT STEPS]\")\n",
        "        print(\"1. Download all files\")\n",
        "        print(\"2. Deploy to your project:\")\n",
        "        print(\"   - best_model_4datasets.pth -> training_experiments/checkpoints/production/\")\n",
        "        print(\"   - best_model.onnx -> ai_edge_app/models/\")\n",
        "        print(\"   - training_results.json -> training_experiments/results/\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CELL 9: FINAL SUMMARY\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results_file = Path('/kaggle/working/checkpoints_production/training_results.json')\n",
        "\n",
        "if results_file.exists():\n",
        "    with open(results_file) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"TRAINING SUMMARY\")\n",
        "    print('='*60)\n",
        "    \n",
        "    print(f\"\\nAccuracy: {results['best_accuracy']:.2f}%\")\n",
        "    print(f\"Training Time: {results['training_time_hours']:.2f} hours\")\n",
        "    print(f\"Best Epoch: {results['best_epoch']}/{results['total_epochs']}\")\n",
        "    \n",
        "    if 'datasets_used' in results:\n",
        "        print(f\"\\nDatasets ({len(results['datasets_used'])}):\")\n",
        "        for ds in results['datasets_used']:\n",
        "            print(f\"  - {ds.upper()}\")\n",
        "    \n",
        "    if 'total_train_images' in results:\n",
        "        print(f\"\\nImages:\")\n",
        "        print(f\"  Train: {results['total_train_images']:,}\")\n",
        "        print(f\"  Test: {results['total_test_images']:,}\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FILES READY\")\n",
        "    print('='*60)\n",
        "    \n",
        "    checkpoint_dir = Path('/kaggle/working/checkpoints_production')\n",
        "    for file in checkpoint_dir.glob('*'):\n",
        "        if file.is_file():\n",
        "            size = file.stat().st_size / (1024*1024)\n",
        "            print(f\"  {file.name} ({size:.1f} MB)\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"NEXT STEPS\")\n",
        "    print('='*60)\n",
        "    print(\"1. Download files (Cell 8)\")\n",
        "    print(\"2. Deploy to project:\")\n",
        "    print(\"   cd 'D:\\\\AI vietnam\\\\Code\\\\nhan dien do tuoi'\")\n",
        "    print(\"   # Copy files to correct locations\")\n",
        "    print(\"3. Test model locally:\")\n",
        "    print(\"   cd ai_edge_app\")\n",
        "    print(\"   python main.py\")\n",
        "    print(\"4. Deploy to production\")\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    \n",
        "    # Final verdict\n",
        "    best_acc = results['best_accuracy']\n",
        "    if best_acc >= 80:\n",
        "        print(\"STATUS: PRODUCTION READY! âœ“\")\n",
        "    elif best_acc >= 78:\n",
        "        print(\"STATUS: NEAR PRODUCTION READY\")\n",
        "    elif best_acc >= 75:\n",
        "        print(\"STATUS: GOOD FOR TESTING\")\n",
        "    else:\n",
        "        print(f\"STATUS: COMPLETED ({best_acc:.2f}%)\")\n",
        "    \n",
        "    print('='*60)\n",
        "    \n",
        "else:\n",
        "    print(\"\\n[ERROR] No results found\")\n",
        "    print(\"[INFO] Please run Cell 5 (Training) first\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
