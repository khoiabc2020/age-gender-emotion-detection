{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production Training on Google Colab - Option B\n",
        "\n",
        "## Target: 78-85% Accuracy\n",
        "\n",
        "### Features:\n",
        "- **4 datasets**: FER2013, UTKFace, RAF-DB, AffectNet (96K images)\n",
        "- **Backbone**: EfficientNet-B2 (pretrained)\n",
        "- **Loss**: Focal Loss + Label Smoothing\n",
        "- **Augmentation**: Advanced (Albumentations + Mixup/Cutmix)\n",
        "- **Regularization**: Dropout 0.5 + Weight Decay 1e-4\n",
        "- **Training**: 100 epochs with early stopping\n",
        "\n",
        "### Estimated Time:\n",
        "- T4 GPU: 10-12 hours\n",
        "- V100 GPU: 4-6 hours\n",
        "- A100 GPU: 2-3 hours\n",
        "\n",
        "### Pre-Run Checklist:\n",
        "- [ ] GPU enabled (Runtime > Change runtime type > T4 GPU)\n",
        "- [ ] kaggle.json file ready (from https://www.kaggle.com/settings)\n",
        "- [ ] Google Drive mounted (for saving results)\n",
        "\n",
        "---\n",
        "\n",
        "## IMPORTANT:\n",
        "Run cells in order. Do NOT skip cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 1: CHECK GPU\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import torch.cuda as cuda\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", cuda.is_available())\n",
        "\n",
        "if cuda.is_available():\n",
        "    print(\"CUDA version:\", torch.version.cuda)\n",
        "    print(\"GPU device:\", cuda.get_device_name(0))\n",
        "    print(f\"GPU memory: {cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU available! Training will be very slow.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 2: CLONE REPOSITORY\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "repo_url = \"https://github.com/khoiabc2020/age-gender-emotion-detection.git\"\n",
        "repo_dir = Path(\"/content/repo\")\n",
        "\n",
        "if repo_dir.exists():\n",
        "    print(\"[INFO] Repository exists, pulling latest changes...\")\n",
        "    !cd /content/repo && git pull\n",
        "else:\n",
        "    print(\"[INFO] Cloning repository...\")\n",
        "    !git clone {repo_url} /content/repo\n",
        "\n",
        "# Change to project directory\n",
        "%cd /content/repo/training_experiments\n",
        "print(\"\\n[OK] Repository ready!\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 3: SETUP KAGGLE API\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "print(\"[INFO] Please upload your kaggle.json file\")\n",
        "print(\"[INFO] Get it from: https://www.kaggle.com/settings\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "if 'kaggle.json' in uploaded:\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !mv kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(\"\\n[OK] Kaggle API configured!\")\n",
        "else:\n",
        "    print(\"\\n[ERROR] kaggle.json not found!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 4: DOWNLOAD 4 DATASETS (PRODUCTION)\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DOWNLOADING 4 DATASETS FOR PRODUCTION TRAINING\")\n",
        "print(\"Total: ~1.5GB | Time: ~20-30 minutes\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install kagglehub if needed\n",
        "%pip install -q kagglehub\n",
        "\n",
        "import kagglehub\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_paths = {}\n",
        "\n",
        "# 1. FER2013 - Emotion Recognition Dataset\n",
        "print(\"\\n[1/4] Downloading FER2013 (Emotion)...\")\n",
        "print(\"      Size: ~60MB | Time: ~3 minutes\")\n",
        "fer2013_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "dataset_paths['fer2013'] = fer2013_path\n",
        "print(f\"      [OK] FER2013: {fer2013_path}\")\n",
        "\n",
        "# 2. UTKFace - Age & Gender Dataset\n",
        "print(\"\\n[2/4] Downloading UTKFace (Age/Gender)...\")\n",
        "print(\"      Size: ~500MB | Time: ~5 minutes\")\n",
        "utkface_path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
        "dataset_paths['utkface'] = utkface_path\n",
        "print(f\"      [OK] UTKFace: {utkface_path}\")\n",
        "\n",
        "# 3. RAF-DB - High-Quality Emotion Dataset (Optional)\n",
        "print(\"\\n[3/4] Downloading RAF-DB (High-Quality Emotion)...\")\n",
        "print(\"      Size: ~200MB | Time: ~5 minutes\")\n",
        "try:\n",
        "    rafdb_datasets = [\n",
        "        \"shuvoalok/raf-db-dataset\",\n",
        "        \"alex1233213/raf-db\"\n",
        "    ]\n",
        "    rafdb_path = None\n",
        "    for dataset in rafdb_datasets:\n",
        "        try:\n",
        "            rafdb_path = kagglehub.dataset_download(dataset)\n",
        "            dataset_paths['rafdb'] = rafdb_path\n",
        "            print(f\"      [OK] RAF-DB: {rafdb_path}\")\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "    if rafdb_path is None:\n",
        "        print(f\"      [WARN] RAF-DB not available, continuing without it\")\n",
        "except Exception as e:\n",
        "    print(f\"      [WARN] RAF-DB error: {e}\")\n",
        "\n",
        "# 4. AffectNet - Large-scale Emotion Dataset (Optional)\n",
        "print(\"\\n[4/4] Downloading AffectNet subset...\")\n",
        "print(\"      Size: ~250MB | Time: ~5 minutes\")\n",
        "try:\n",
        "    affectnet_datasets = [\n",
        "        \"noamsegal/affectnet-training-data\",\n",
        "        \"tom99763/affectnet-cnn-validation\"\n",
        "    ]\n",
        "    affectnet_path = None\n",
        "    for dataset in affectnet_datasets:\n",
        "        try:\n",
        "            affectnet_path = kagglehub.dataset_download(dataset)\n",
        "            dataset_paths['affectnet'] = affectnet_path\n",
        "            print(f\"      [OK] AffectNet: {affectnet_path}\")\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "    if affectnet_path is None:\n",
        "        print(f\"      [WARN] AffectNet not available, continuing without it\")\n",
        "except Exception as e:\n",
        "    print(f\"      [WARN] AffectNet error: {e}\")\n",
        "\n",
        "# Save paths to JSON for production training\n",
        "paths_file = '/content/dataset_paths.json'\n",
        "with open(paths_file, 'w') as f:\n",
        "    json.dump(dataset_paths, f, indent=2)\n",
        "\n",
        "# Also save legacy .txt format for compatibility\n",
        "legacy_file = '/content/dataset_paths.txt'\n",
        "with open(legacy_file, 'w') as f:\n",
        "    f.write(f\"FER2013: {dataset_paths['fer2013']}\\n\")\n",
        "    f.write(f\"UTKFace: {dataset_paths['utkface']}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"[OK] DATASET DOWNLOAD COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTotal datasets: {len(dataset_paths)}\")\n",
        "for name, path in dataset_paths.items():\n",
        "    print(f\"  - {name.upper()}: {path}\")\n",
        "print(f\"\\n[INFO] Paths saved to: {paths_file}\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 5: INSTALL PRODUCTION DEPENDENCIES\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INSTALLING PRODUCTION DEPENDENCIES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n[INFO] Installing packages for production training...\")\n",
        "print(\"[INFO] This includes: timm, albumentations, imgaug, etc.\")\n",
        "print(\"[INFO] Time: ~2-3 minutes\\n\")\n",
        "\n",
        "%pip install -q timm albumentations imgaug tensorboard onnx onnxscript onnxruntime torchmetrics opencv-python\n",
        "\n",
        "print(\"\\n[OK] All production dependencies installed!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 6: RUN PRODUCTION TRAINING\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PRODUCTION TRAINING - OPTION B\")\n",
        "print(\"Target: 78-85% Accuracy\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Display configuration\n",
        "print(\"\\n[CONFIG] Training Configuration:\")\n",
        "print(\"  Backbone: EfficientNet-B2\")\n",
        "print(\"  Epochs: 100 (with early stopping)\")\n",
        "print(\"  Batch Size: 64\")\n",
        "print(\"  Learning Rate: 0.0001\")\n",
        "print(\"  Optimizer: AdamW\")\n",
        "print(\"  Loss: Focal Loss + Label Smoothing\")\n",
        "print(\"  Augmentation: Advanced (Albumentations + Mixup/Cutmix)\")\n",
        "print(\"  Regularization: Dropout 0.5 + Weight Decay 1e-4\")\n",
        "\n",
        "# Estimate time\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    if 'T4' in gpu_name:\n",
        "        print(\"\\n[INFO] Using T4 GPU - Estimated time: 10-12 hours\")\n",
        "    elif 'V100' in gpu_name:\n",
        "        print(\"\\n[INFO] Using V100 GPU - Estimated time: 4-6 hours\")\n",
        "    elif 'A100' in gpu_name:\n",
        "        print(\"\\n[INFO] Using A100 GPU - Estimated time: 2-3 hours\")\n",
        "\n",
        "# Verify files\n",
        "if not os.path.exists('/content/dataset_paths.json'):\n",
        "    print(\"[ERROR] Dataset paths file not found!\")\n",
        "    print(\"[INFO] Please run Cell 4 first\")\n",
        "    raise FileNotFoundError(\"Dataset paths required\")\n",
        "\n",
        "print(\"\\n[START] Starting production training...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Pull latest code\n",
        "%cd /content/repo\n",
        "!git pull\n",
        "%cd /content/repo/training_experiments\n",
        "\n",
        "# Run production training (choose one script)\n",
        "!python train_production.py \\\n",
        "    --data_paths /content/dataset_paths.json \\\n",
        "    --epochs 100 \\\n",
        "    --batch_size 64 \\\n",
        "    --lr 0.0001 \\\n",
        "    --patience 15 \\\n",
        "    --save_dir /content/checkpoints_production\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"[OK] TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 7: EVALUATE RESULTS\n",
        "# ============================================================\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load results\n",
        "results_file = Path('/content/checkpoints_production/training_results.json')\n",
        "\n",
        "if results_file.exists():\n",
        "    with open(results_file) as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    print(f\"\\n[SUCCESS] Training Completed!\")\n",
        "    print(f\"\\nBest Validation Accuracy: {results.get('best_accuracy', 0):.2f}%\")\n",
        "    print(f\"Best Epoch: {results.get('best_epoch', 'N/A')}\")\n",
        "    print(f\"Total Epochs: {results.get('total_epochs', 'N/A')}\")\n",
        "    \n",
        "    # Check if target achieved\n",
        "    best_acc = results.get('best_accuracy', 0)\n",
        "    if best_acc >= 78:\n",
        "        print(\"\\n[OK] TARGET ACHIEVED! (78-85%)\")\n",
        "        print(\"Model is production-ready!\")\n",
        "    elif best_acc >= 75:\n",
        "        print(\"\\n[OK] Good accuracy, close to target\")\n",
        "    else:\n",
        "        print(\"\\n[WARN] Below target\")\n",
        "    \n",
        "    print(f\"\\n[INFO] Model saved to:\")\n",
        "    print(f\"  - /content/checkpoints_production/best_model.pth\")\n",
        "    \n",
        "else:\n",
        "    print(\"\\n[WARN] Results file not found\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CELL 8: SAVE TO GOOGLE DRIVE\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directory\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "drive_dir = Path(f'/content/drive/MyDrive/SmartRetailAI_Models/production_{timestamp}')\n",
        "drive_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SAVING TO GOOGLE DRIVE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Copy files\n",
        "files_to_copy = [\n",
        "    ('/content/checkpoints_production/best_model.pth', 'best_model_production.pth'),\n",
        "    ('/content/checkpoints_production/training_results.json', 'training_results.json'),\n",
        "    ('/content/dataset_paths.json', 'dataset_paths.json')\n",
        "]\n",
        "\n",
        "copied = []\n",
        "total_size = 0\n",
        "\n",
        "for src, dst_name in files_to_copy:\n",
        "    src_path = Path(src)\n",
        "    if src_path.exists():\n",
        "        dst_path = drive_dir / dst_name\n",
        "        shutil.copy2(src_path, dst_path)\n",
        "        size = dst_path.stat().st_size / (1024*1024)\n",
        "        total_size += size\n",
        "        print(f\"  [OK] {dst_name} ({size:.1f} MB)\")\n",
        "        copied.append(dst_name)\n",
        "    else:\n",
        "        print(f\"  [WARN] {dst_name} - not found\")\n",
        "\n",
        "print(f\"\\n[OK] Saved {len(copied)} files ({total_size:.1f} MB total)\")\n",
        "print(f\"\\n[INFO] Location: {drive_dir}\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}