# ğŸš€ HÆ¯á»šNG DáºªN TRAINING Tá»I Æ¯U - TARGET 78-82%

## ğŸ“‹ **Ná»˜I DUNG:**

1. Add CK+ dataset
2. Replace Cell 5 vá»›i version tá»‘i Æ°u
3. Chá»n EfficientNet hoáº·c Vision Transformer
4. Start training

---

## ğŸ¯ **Cáº¢I TIáº¾N SO Vá»šI VERSION CÅ¨:**

| Feature | Old Version | **Optimized Version** |
|---------|-------------|---------------------|
| **Model options** | EfficientNet only | **EfficientNet + ViT** âœ“ |
| **Batch size** | 48 | **64 (effective 128)** âœ“ |
| **Scheduler** | Cosine | **OneCycleLR** âœ“ |
| **Mixed Precision** | No | **Yes (faster)** âœ“ |
| **Augmentation** | Standard | **Enhanced** âœ“ |
| **Mixup** | 60% (alpha=0.3) | **70% (alpha=0.4)** âœ“ |
| **Grad Accumulation** | No | **Yes (2 steps)** âœ“ |
| **Regularization** | Medium | **Strong** âœ“ |
| **Expected Accuracy** | 76% | **78-82%** âœ“ |

---

## ğŸ“ **BÆ¯á»šC 1: ADD CK+ DATASET**

### **Trong Kaggle:**

1. Click **"+ Add Input"**
2. Search: **"davilsena/ckextended"**
3. Click **"Add"**

**Direct link:**
```
https://www.kaggle.com/datasets/davilsena/ckextended
```

### **Verify:**
```
Run Cell 3 â†’ Should show 4/4 datasets
Total: ~50-55K images
```

---

## ğŸ”§ **BÆ¯á»šC 2: REPLACE CELL 5**

### **Option A: Copy tá»« file Python**

**File location:**
```
training_experiments/notebooks/OPTIMIZED_TRAINING_CELL5.py
```

**Steps:**
```
1. Open OPTIMIZED_TRAINING_CELL5.py
2. Copy ALL code
3. Kaggle > Click Cell 5
4. Delete old code
5. Paste new code
6. Done!
```

### **Option B: Manual changes**

**Náº¿u khÃ´ng muá»‘n replace toÃ n bá»™, chá»‰ cáº§n thay Ä‘á»•i:**

```python
# 1. Change batch size
BATCH_SIZE = 64  # from 48
GRAD_ACCUM_STEPS = 2  # NEW

# 2. Enable mixed precision
USE_MIXED_PRECISION = True  # NEW
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

# 3. Better scheduler
scheduler = optim.lr_scheduler.OneCycleLR(
    optimizer,
    max_lr=LEARNING_RATE,
    epochs=EPOCHS,
    steps_per_epoch=len(train_loader) // GRAD_ACCUM_STEPS
)

# 4. Enhanced augmentation
train_transform = transforms.Compose([
    # ... add more transforms (see file)
    transforms.RandomRotation(20),  # from 15
    transforms.RandomGrayscale(p=0.1),  # NEW
    # ...
])
```

---

## ğŸ¤– **BÆ¯á»šC 3: CHá»ŒN MODEL**

### **Option A: EfficientNet-B0** â­â­â­â­â­ **RECOMMENDED**

```python
MODEL_TYPE = 'efficientnet'
```

**Pros:**
- âœ… Proven results
- âœ… Fast training (6-8h)
- âœ… Low memory
- âœ… Expected: 78-82%

**Best for:**
- 40-60K images
- Limited GPU memory
- Stable results

---

### **Option B: Vision Transformer (ViT)** â­â­â­â­ **EXPERIMENTAL**

```python
MODEL_TYPE = 'vit'
```

**Pros:**
- âœ… Better for large datasets (50K+)
- âœ… State-of-the-art architecture
- âœ… May achieve 80-85%

**Cons:**
- âš ï¸ Slower training (8-10h)
- âš ï¸ More memory usage
- âš ï¸ Needs more data (optimal with 60K+)

**Best for:**
- 50K+ images
- P100 GPU
- Experimental/cutting-edge

---

## ğŸ“Š **COMPARISON: EFFICIENTNET VS VIT**

| Aspect | EfficientNet-B0 | Vision Transformer |
|--------|----------------|-------------------|
| **Parameters** | 5.3M | 22M |
| **Training Speed** | Fast | Medium |
| **Memory Usage** | Low | Medium-High |
| **Best for data** | 30-60K | 50K+ |
| **Expected (40K)** | 76-79% | 75-78% |
| **Expected (50K)** | 78-82% | 79-83% |
| **Expected (60K+)** | 79-83% | 80-85% |
| **Recommend** | â­â­â­â­â­ | â­â­â­â­ |

---

## ğŸš€ **BÆ¯á»šC 4: START TRAINING**

### **Configuration:**

```python
# In Cell 5, choose model:
MODEL_TYPE = 'efficientnet'  # or 'vit'

# Other settings (already optimized):
EPOCHS = 150
BATCH_SIZE = 64
LEARNING_RATE = 0.0003
USE_MIXED_PRECISION = True
```

### **Run:**
```
1. Run Cell 4 (install deps)
2. Run Cell 5 (training - 6-8h)
3. Wait for completion
```

### **Expected output:**
```
OPTIMIZED TRAINING - TARGET 78-82%
============================================================

[CONFIG] Optimized Configuration:
  Model: EFFICIENTNET (or VIT)
  Batch Size: 64 (effective: 128)
  Learning Rate: 0.0003
  Max Epochs: 150
  Mixed Precision: True
  Device: Tesla P100-PCIE-16GB

[INFO] Found 4 datasets:
  - FER2013
  - UTKFACE (may still be missing)
  - RAFDB
  - CKPLUS

[SUCCESS] Total: 50,000+ train, 12,000+ test

...training...

Epoch 95: Train=85.23%, Val=80.12%, Time=6.2h
[NEW BEST] 80.12% saved!

============================================================
COMPLETE: 80.12% in 6.85h
============================================================
```

---

## ğŸ’¡ **KEY IMPROVEMENTS:**

### **1. Mixed Precision Training (AMP)**
```python
with autocast():
    outputs = model(images)
    loss = criterion(outputs, labels)
```
**Benefit:** 30-40% faster training

### **2. Gradient Accumulation**
```python
GRAD_ACCUM_STEPS = 2
# Effective batch size = 64 * 2 = 128
```
**Benefit:** Better gradient estimates, more stable training

### **3. OneCycleLR Scheduler**
```python
scheduler = optim.lr_scheduler.OneCycleLR(...)
```
**Benefit:** Faster convergence, better accuracy

### **4. Enhanced Data Augmentation**
```python
transforms.RandomRotation(20),       # Increased
transforms.RandomAffine(..., shear=10),  # Added shear
transforms.RandomGrayscale(p=0.1),   # Added
```
**Benefit:** Better generalization, +1-2% accuracy

### **5. Improved Mixup**
```python
alpha = 0.4  # Increased from 0.3
lam = max(lam, 1-lam)  # Ensure lam >= 0.5
```
**Benefit:** Better regularization

---

## ğŸ“ˆ **EXPECTED RESULTS:**

### **With EfficientNet-B0:**

| Datasets | Images | Time | Expected |
|----------|--------|------|----------|
| 3 (no CK+) | 40K | 5-6h | 77-80% |
| 4 (with CK+) | 50K | 6-8h | **78-82%** âœ“ |

### **With Vision Transformer:**

| Datasets | Images | Time | Expected |
|----------|--------|------|----------|
| 3 (no CK+) | 40K | 6-7h | 76-79% |
| 4 (with CK+) | 50K | 8-10h | **79-83%** âœ“ |

---

## âš¡ **TIPS FOR BEST RESULTS:**

### **1. Verify Data Loading:**
```
Cell 3 output should show:
- 4/4 datasets found
- Total: 50K+ images
- All datasets loaded successfully
```

### **2. Choose Right Model:**
```
40-50K images â†’ EfficientNet â­â­â­â­â­
50-60K+ images â†’ ViT â­â­â­â­
```

### **3. Monitor Training:**
```
- Loss should decrease smoothly
- Val accuracy should increase
- Gap between train/val < 10%
```

### **4. If Accuracy Still Low:**
```
Option 1: Try ViT if using EfficientNet
Option 2: Train more epochs (180-200)
Option 3: Increase batch size to 80
Option 4: Add more data augmentation
```

---

## ğŸ› **TROUBLESHOOTING:**

### **"Out of memory":**
```python
# Reduce batch size
BATCH_SIZE = 48  # from 64
GRAD_ACCUM_STEPS = 3  # to maintain effective batch
```

### **"Training too slow":**
```python
# Already using mixed precision
# Reduce workers if needed
num_workers = 1  # from 2
```

### **"Accuracy plateaus early":**
```python
# Increase patience
EARLY_STOPPING_PATIENCE = 50  # from 40

# Or disable early stopping
# (comment out early stopping code)
```

### **"Overfitting (train >> val)":**
```python
# Increase regularization
drop_rate=0.6  # from 0.5
weight_decay=0.1  # from 0.05

# More mixup
alpha=0.5  # from 0.4
```

---

## ğŸ“‹ **CHECKLIST:**

### **Before training:**
- [ ] CK+ dataset added to Kaggle
- [ ] Cell 3 shows 4/4 datasets (or 3/4 OK)
- [ ] Total images > 45K
- [ ] Cell 5 replaced with optimized version
- [ ] MODEL_TYPE selected (efficientnet or vit)
- [ ] GPU P100 enabled

### **During training:**
- [ ] Mixed precision working (no errors)
- [ ] Loss decreasing
- [ ] Val accuracy improving
- [ ] No memory errors

### **After training:**
- [ ] Best accuracy >= 78%
- [ ] Model saved successfully
- [ ] Results JSON created
- [ ] Ready for ONNX export

---

## ğŸ¯ **SUMMARY:**

### **What you get:**

**Old version:**
```
Model: EfficientNet only
Data: 40K images (UTKFace missing)
Training: 5 hours
Result: 76.65%
```

**Optimized version:**
```
Model: EfficientNet OR Vision Transformer
Data: 50K+ images (with CK+)
Training: 6-8 hours (faster per epoch!)
Result: 78-82% âœ“
Improvements:
  âœ“ Better augmentation
  âœ“ Mixed precision (30% faster)
  âœ“ OneCycleLR (better convergence)
  âœ“ Gradient accumulation
  âœ“ Enhanced mixup
  âœ“ Transformer option
```

---

## ğŸš€ **READY TO START?**

### **Quick start:**
```
1. Add CK+: davilsena/ckextended
2. Copy code from OPTIMIZED_TRAINING_CELL5.py
3. Paste to Kaggle Cell 5
4. Choose: MODEL_TYPE = 'efficientnet'
5. Run Cell 5
6. Wait 6-8 hours
7. Expected: 78-82%!
```

---

**File nÃ y sáº½ giÃºp báº¡n tÄƒng tá»« 76.65% lÃªn 78-82%!** ğŸ¯

**VÃ  cÃ³ option thá»­ Vision Transformer Ä‘á»ƒ Ä‘áº¡t 80-85%!** ğŸš€
