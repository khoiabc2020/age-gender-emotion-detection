{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Training Model tr√™n Google Colab\n",
        "\n",
        "Notebook n√†y gi√∫p b·∫°n train model tr√™n Google Colab v·ªõi GPU mi·ªÖn ph√≠.\n",
        "\n",
        "## üìã H∆∞·ªõng d·∫´n:\n",
        "1. Ch·∫°y c√°c cell theo th·ª© t·ª±\n",
        "2. Upload d·ªØ li·ªáu ho·∫∑c s·ª≠ d·ª•ng Google Drive\n",
        "3. Ch·ªçn GPU runtime: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "4. Ch·∫°y training v√† l∆∞u k·∫øt qu·∫£ v·ªÅ Google Drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng v√† dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install -q albumentations tqdm tensorboard onnx onnxruntime\n",
        "%pip install -q pandas numpy Pillow opencv-python\n",
        "%pip install -q kagglehub\n",
        "\n",
        "print(\"‚úÖ ƒê√£ c√†i ƒë·∫∑t xong c√°c th∆∞ vi·ªán!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Ki·ªÉm tra GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.cuda as cuda\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {cuda.is_available()}\")\n",
        "if cuda.is_available():\n",
        "    print(f\"CUDA version: {cuda.version()}\")\n",
        "    print(f\"GPU device: {cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Kh√¥ng c√≥ GPU! Vui l√≤ng ch·ªçn GPU runtime:\")\n",
        "    print(\"   Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mount Google Drive (ƒë·ªÉ l∆∞u k·∫øt qu·∫£)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ tr√™n Drive\n",
        "DRIVE_RESULTS_DIR = '/content/drive/MyDrive/age_gender_emotion_training'\n",
        "os.makedirs(DRIVE_RESULTS_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ ƒê√£ mount Google Drive\")\n",
        "print(f\"üìÅ K·∫øt qu·∫£ s·∫Ω l∆∞u t·∫°i: {DRIVE_RESULTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Upload code t·ª´ m√°y t√≠nh\n",
        "\n",
        "**C√°ch 1: Upload tr·ª±c ti·∫øp (khuy·∫øn ngh·ªã)**\n",
        "- Click v√†o icon folder b√™n tr√°i\n",
        "- Upload th∆∞ m·ª•c `training_experiments` v√†o `/content/`\n",
        "- Ho·∫∑c upload file zip v√† gi·∫£i n√©n\n",
        "\n",
        "**C√°ch 2: Clone t·ª´ GitHub (n·∫øu code ƒë√£ c√≥ tr√™n GitHub)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o th∆∞ m·ª•c project\n",
        "!mkdir -p /content/project\n",
        "%cd /content/project\n",
        "\n",
        "# Option 1: Clone t·ª´ GitHub (n·∫øu c√≥)\n",
        "# !git clone https://github.com/your-username/your-repo.git .\n",
        "# %cd training_experiments\n",
        "\n",
        "# Option 2: Upload code t·ª´ m√°y t√≠nh\n",
        "# S·ª≠ d·ª•ng file uploader ·ªü cell ti·∫øp theo ho·∫∑c upload th·ªß c√¥ng qua file browser\n",
        "\n",
        "print(\"üìÅ ƒê√£ t·∫°o th∆∞ m·ª•c project t·∫°i /content/project\")\n",
        "print(\"\\nüì§ B√¢y gi·ªù h√£y upload code:\")\n",
        "print(\"   1. Click v√†o icon folder b√™n tr√°i (üìÅ)\")\n",
        "print(\"   2. Upload th∆∞ m·ª•c training_experiments v√†o /content/project/\")\n",
        "print(\"   3. Ho·∫∑c upload file zip v√† gi·∫£i n√©n ·ªü cell ti·∫øp theo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload v√† gi·∫£i n√©n code (n·∫øu upload d·∫°ng zip)\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"üì§ Upload file zip ch·ª©a code training_experiments.zip\")\n",
        "print(\"   (B·ªè qua n·∫øu ƒë√£ upload th∆∞ m·ª•c tr·ª±c ti·∫øp)\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Gi·∫£i n√©n n·∫øu c√≥ file zip\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"\\nüì¶ ƒêang gi·∫£i n√©n {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/project')\n",
        "        print(f\"‚úÖ ƒê√£ gi·∫£i n√©n v√†o /content/project/\")\n",
        "        os.remove(filename)  # X√≥a file zip sau khi gi·∫£i n√©n\n",
        "\n",
        "# Ki·ªÉm tra c·∫•u tr√∫c\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "project_dir = Path('/content/project/training_experiments')\n",
        "if project_dir.exists():\n",
        "    print(f\"\\n‚úÖ ƒê√£ t√¨m th·∫•y training_experiments t·∫°i: {project_dir}\")\n",
        "    print(f\"   C√°c file quan tr·ªçng:\")\n",
        "    files_to_check = [\n",
        "        'train_week2_lightweight.py',\n",
        "        'src/data/dataset.py',\n",
        "        'src/models/mobileone.py'\n",
        "    ]\n",
        "    for f in files_to_check:\n",
        "        file_path = project_dir / f\n",
        "        print(f\"   {'‚úÖ' if file_path.exists() else '‚ùå'} {f}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Ch∆∞a t√¨m th·∫•y training_experiments!\")\n",
        "    print(f\"   H√£y upload code v√†o /content/project/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Upload d·ªØ li·ªáu training\n",
        "\n",
        "**C√≥ 3 c√°ch ƒë·ªÉ c√≥ d·ªØ li·ªáu:**\n",
        "\n",
        "1. **Upload t·ª´ m√°y t√≠nh** (cho dataset nh·ªè < 2GB)\n",
        "2. **Download t·ª´ Kaggle** (khuy·∫øn ngh·ªã cho dataset l·ªõn)\n",
        "3. **S·ª≠ d·ª•ng t·ª´ Google Drive** (n·∫øu ƒë√£ upload tr∆∞·ªõc ƒë√≥)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option A: Upload t·ª´ m√°y t√≠nh (cho dataset nh·ªè)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload file zip ch·ª©a d·ªØ li·ªáu ƒë√£ processed\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üì§ H√£y upload file zip ch·ª©a d·ªØ li·ªáu (data_processed.zip)\")\n",
        "print(\"   File zip n√™n c√≥ c·∫•u tr√∫c: processed/train/, processed/val/, processed/test/\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Gi·∫£i n√©n\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"\\nüì¶ ƒêang gi·∫£i n√©n {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content/project/training_experiments/data')\n",
        "        print(f\"‚úÖ ƒê√£ gi·∫£i n√©n v√†o /content/project/training_experiments/data/\")\n",
        "        os.remove(filename)  # X√≥a file zip sau khi gi·∫£i n√©n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Download dataset t·ª´ Kaggle (khuy·∫øn ngh·ªã)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t kaggle API\n",
        "%pip install -q kaggle\n",
        "\n",
        "# Upload kaggle.json t·ª´ m√°y t√≠nh\n",
        "# L·∫•y t·ª´: https://www.kaggle.com/settings -> API -> Create New Token\n",
        "print(\"üì§ Upload file kaggle.json (t·ª´ ~/.kaggle/kaggle.json)\")\n",
        "print(\"   B·ªè qua n·∫øu ƒë√£ upload tr∆∞·ªõc ƒë√≥\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Di chuy·ªÉn kaggle.json v√†o ƒë√∫ng v·ªã tr√≠\n",
        "if 'kaggle.json' in uploaded:\n",
        "    import os\n",
        "    os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
        "    import shutil\n",
        "    shutil.move('kaggle.json', os.path.expanduser('~/.kaggle/kaggle.json'))\n",
        "    os.chmod(os.path.expanduser('~/.kaggle/kaggle.json'), 0o600)\n",
        "    print(\"‚úÖ ƒê√£ setup Kaggle API\")\n",
        "\n",
        "# Download datasets\n",
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('/content/project/training_experiments/data')\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"\\nüì• ƒêang download datasets...\")\n",
        "print(\"   (C√≥ th·ªÉ m·∫•t v√†i ph√∫t)\")\n",
        "\n",
        "# Download UTKFace\n",
        "try:\n",
        "    utkface_path = kagglehub.dataset_download(\"jangedoo/utkface-new\")\n",
        "    print(f\"‚úÖ UTKFace: {utkface_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  L·ªói download UTKFace: {e}\")\n",
        "\n",
        "# Download FER2013\n",
        "try:\n",
        "    fer2013_path = kagglehub.dataset_download(\"msambare/fer2013\")\n",
        "    print(f\"‚úÖ FER2013: {fer2013_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  L·ªói download FER2013: {e}\")\n",
        "\n",
        "print(\"\\n‚úÖ Ho√†n t·∫•t download datasets\")\n",
        "print(\"\\n‚ö†Ô∏è  L∆∞u √Ω: C·∫ßn ch·∫°y script preprocess ƒë·ªÉ t·∫°o train/val/test splits\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option C: S·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy d·ªØ li·ªáu t·ª´ Google Drive\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DRIVE_DATA_DIR = '/content/drive/MyDrive/age_gender_emotion_data'\n",
        "LOCAL_DATA_DIR = Path('/content/project/training_experiments/data/processed')\n",
        "\n",
        "if os.path.exists(DRIVE_DATA_DIR):\n",
        "    print(f\"üìÅ Copy d·ªØ li·ªáu t·ª´ {DRIVE_DATA_DIR}...\")\n",
        "    shutil.copytree(DRIVE_DATA_DIR, LOCAL_DATA_DIR, dirs_exist_ok=True)\n",
        "    print(f\"‚úÖ ƒê√£ copy d·ªØ li·ªáu v√†o {LOCAL_DATA_DIR}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu t·∫°i {DRIVE_DATA_DIR}\")\n",
        "    print(\"   H√£y upload d·ªØ li·ªáu v√†o Google Drive tr∆∞·ªõc\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Ki·ªÉm tra v√† chu·∫©n b·ªã d·ªØ li·ªáu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ s·∫µn s√†ng ch∆∞a\n",
        "from pathlib import Path\n",
        "\n",
        "data_dir = Path('/content/project/training_experiments/data/processed')\n",
        "\n",
        "train_dir = data_dir / 'train'\n",
        "val_dir = data_dir / 'val'\n",
        "test_dir = data_dir / 'test'\n",
        "\n",
        "print(\"üìä Ki·ªÉm tra d·ªØ li·ªáu:\")\n",
        "print(f\"   Train: {train_dir.exists()}\")\n",
        "print(f\"   Val: {val_dir.exists()}\")\n",
        "print(f\"   Test: {test_dir.exists()}\")\n",
        "\n",
        "if train_dir.exists():\n",
        "    # ƒê·∫øm s·ªë ·∫£nh\n",
        "    train_images = list(train_dir.glob('**/*.jpg')) + list(train_dir.glob('**/*.png'))\n",
        "    val_images = list(val_dir.glob('**/*.jpg')) + list(val_dir.glob('**/*.png')) if val_dir.exists() else []\n",
        "    test_images = list(test_dir.glob('**/*.jpg')) + list(test_dir.glob('**/*.png')) if test_dir.exists() else []\n",
        "    \n",
        "    print(f\"\\nüì∏ S·ªë l∆∞·ª£ng ·∫£nh:\")\n",
        "    print(f\"   Train: {len(train_images)}\")\n",
        "    print(f\"   Val: {len(val_images)}\")\n",
        "    print(f\"   Test: {len(test_images)}\")\n",
        "    \n",
        "    if len(train_images) == 0:\n",
        "        print(\"\\n‚ö†Ô∏è  Ch∆∞a c√≥ ·∫£nh trong th∆∞ m·ª•c train!\")\n",
        "        print(\"   C·∫ßn ch·∫°y script preprocess tr∆∞·ªõc\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Ch∆∞a c√≥ d·ªØ li·ªáu processed!\")\n",
        "    print(\"   C·∫ßn download v√† preprocess d·ªØ li·ªáu tr∆∞·ªõc\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Ch·∫°y Training\n",
        "\n",
        "### Training v·ªõi c·∫•u h√¨nh m·∫∑c ƒë·ªãnh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Th√™m project v√†o Python path\n",
        "project_dir = Path('/content/project/training_experiments')\n",
        "sys.path.insert(0, str(project_dir))\n",
        "\n",
        "# Chuy·ªÉn v√†o th∆∞ m·ª•c training\n",
        "os.chdir(project_dir)\n",
        "\n",
        "# Import v√† ch·∫°y training\n",
        "print(\"üöÄ B·∫Øt ƒë·∫ßu training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ch·∫°y training script\n",
        "cmd = \"\"\"python train_week2_lightweight.py --data_dir data/processed --epochs 50 --batch_size 32 --lr 1e-3 --use_distillation --use_qat --save_dir checkpoints/week2_colab --num_workers 2\"\"\"\n",
        "os.system(cmd)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Training ho√†n t·∫•t!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training v·ªõi c·∫•u h√¨nh t√πy ch·ªânh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T√πy ch·ªânh c√°c tham s·ªë training\n",
        "EPOCHS = 50          # S·ªë epochs\n",
        "BATCH_SIZE = 32      # Batch size (tƒÉng n·∫øu GPU ƒë·ªß m·∫°nh)\n",
        "LEARNING_RATE = 1e-3 # Learning rate\n",
        "USE_QAT = True       # Quantization-Aware Training\n",
        "USE_DISTILLATION = True  # Knowledge Distillation\n",
        "\n",
        "print(f\"‚öôÔ∏è  C·∫•u h√¨nh training:\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   QAT: {USE_QAT}\")\n",
        "print(f\"   Distillation: {USE_DISTILLATION}\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Build command\n",
        "cmd = f\"python train_week2_lightweight.py --data_dir data/processed --epochs {EPOCHS} --batch_size {BATCH_SIZE} --lr {LEARNING_RATE} --save_dir checkpoints/week2_colab --num_workers 2\"\n",
        "\n",
        "if USE_QAT:\n",
        "    cmd += \" --use_qat\"\n",
        "if USE_DISTILLATION:\n",
        "    cmd += \" --use_distillation\"\n",
        "\n",
        "# Ch·∫°y training\n",
        "os.system(cmd)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Training ho√†n t·∫•t!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. L∆∞u k·∫øt qu·∫£ v·ªÅ Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# T·∫°o th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ v·ªõi timestamp\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "results_dir = Path(DRIVE_RESULTS_DIR) / f'training_{timestamp}'\n",
        "results_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Copy checkpoints\n",
        "checkpoint_dir = Path('/content/project/training_experiments/checkpoints/week2_colab')\n",
        "if checkpoint_dir.exists():\n",
        "    print(f\"üì¶ Copy checkpoints...\")\n",
        "    shutil.copytree(checkpoint_dir, results_dir / 'checkpoints', dirs_exist_ok=True)\n",
        "    print(f\"‚úÖ ƒê√£ copy checkpoints\")\n",
        "\n",
        "# Copy logs\n",
        "logs_dir = checkpoint_dir / 'logs'\n",
        "if logs_dir.exists():\n",
        "    print(f\"üìä Copy logs...\")\n",
        "    shutil.copytree(logs_dir, results_dir / 'logs', dirs_exist_ok=True)\n",
        "    print(f\"‚úÖ ƒê√£ copy logs\")\n",
        "\n",
        "# Copy ONNX model\n",
        "onnx_file = checkpoint_dir / 'mobileone_multitask.onnx'\n",
        "if onnx_file.exists():\n",
        "    print(f\"üìÑ Copy ONNX model...\")\n",
        "    shutil.copy2(onnx_file, results_dir / 'mobileone_multitask.onnx')\n",
        "    print(f\"‚úÖ ƒê√£ copy ONNX model\")\n",
        "\n",
        "print(f\"\\n‚úÖ T·∫•t c·∫£ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i:\")\n",
        "print(f\"   {results_dir}\")\n",
        "print(f\"\\nüìÅ B·∫°n c√≥ th·ªÉ t·∫£i v·ªÅ t·ª´ Google Drive sau khi training xong\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Xem k·∫øt qu·∫£ training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load v√† hi·ªÉn th·ªã k·∫øt qu·∫£\n",
        "import torch\n",
        "from pathlib import Path\n",
        "\n",
        "checkpoint_path = Path('/content/project/training_experiments/checkpoints/week2_colab/best_model.pth')\n",
        "\n",
        "if checkpoint_path.exists():\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "    \n",
        "    print(\"üìä K·∫øt qu·∫£ training:\")\n",
        "    print(f\"   Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
        "    \n",
        "    if 'val_metrics' in checkpoint:\n",
        "        metrics = checkpoint['val_metrics']\n",
        "        print(f\"   Gender Accuracy: {metrics.get('gender_acc', 0):.4f}\")\n",
        "        print(f\"   Age MAE: {metrics.get('age_mae', 0):.4f}\")\n",
        "        print(f\"   Emotion Accuracy: {metrics.get('emotion_acc', 0):.4f}\")\n",
        "    \n",
        "    if 'best_val_acc' in checkpoint:\n",
        "        print(f\"   Best Validation Accuracy: {checkpoint['best_val_acc']:.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Ch∆∞a t√¨m th·∫•y checkpoint\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. TensorBoard (xem bi·ªÉu ƒë·ªì training)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kh·ªüi ƒë·ªông TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/project/training_experiments/checkpoints/week2_colab/logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù L∆∞u √Ω quan tr·ªçng:\n",
        "\n",
        "1. **GPU Runtime**: Lu√¥n ch·ªçn GPU runtime ƒë·ªÉ training nhanh h∆°n\n",
        "   - Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\n",
        "\n",
        "2. **Th·ªùi gian training**: \n",
        "   - Colab free: ~12 gi·ªù/ng√†y, c√≥ th·ªÉ b·ªã ng·∫Øt k·∫øt n·ªëi\n",
        "   - Colab Pro: ~24 gi·ªù/ng√†y, ∆∞u ti√™n GPU t·ªët h∆°n\n",
        "\n",
        "3. **L∆∞u k·∫øt qu·∫£**: Lu√¥n l∆∞u k·∫øt qu·∫£ v·ªÅ Google Drive ƒë·ªÉ kh√¥ng b·ªã m·∫•t khi session h·∫øt h·∫°n\n",
        "\n",
        "4. **D·ªØ li·ªáu l·ªõn**: N·∫øu dataset qu√° l·ªõn, n√™n upload l√™n Google Drive tr∆∞·ªõc, sau ƒë√≥ mount v√† copy\n",
        "\n",
        "5. **Resume training**: N·∫øu training b·ªã gi√°n ƒëo·∫°n, c√≥ th·ªÉ load checkpoint v√† ti·∫øp t·ª•c:\n",
        "   ```python\n",
        "   checkpoint = torch.load('checkpoints/week2_colab/best_model.pth')\n",
        "   model.load_state_dict(checkpoint['model_state_dict'])\n",
        "   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "   start_epoch = checkpoint['epoch'] + 1\n",
        "   ```\n",
        "\n",
        "6. **T·ªëi ∆∞u cho Colab**:\n",
        "   - Batch size: 32-64 (t√πy GPU)\n",
        "   - num_workers: 2-4\n",
        "   - S·ª≠ d·ª•ng mixed precision (t·ª± ƒë·ªông trong code)\n",
        "   - Gi·∫£m epochs n·∫øu mu·ªën test nhanh\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
